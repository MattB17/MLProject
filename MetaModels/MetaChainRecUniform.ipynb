{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Models - Uniform\n",
    "\n",
    "In this notebook we prototype a set of Meta Models. In particular, we use the s-values trained using the chainRec algorithm with uniform sampling. Then we use these as input features to our model. These are combined with book level features in the hopes to enhance the predictive accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import scipy.sparse as sp\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading S-Values\n",
    "\n",
    "We load the S-Values from the ChainRec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING_DIR = '../mappings/'\n",
    "\n",
    "s_values_df = pd.read_csv(MAPPING_DIR+\"goodreads_s_values_uniform.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['user_number', 'item_number', 's1', 's2', 's3', 's4']\n",
    "\n",
    "s_values_df = s_values_df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_values_df['user_number'] = s_values_df['user_number'].apply(lambda x: str(x))\n",
    "s_values_df['item_number'] = s_values_df['item_number'].apply(lambda x: str(x))\n",
    "s_values_df['user_item_id'] = s_values_df['user_number'] + \"-\" + s_values_df['item_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DATA_DIR = \"../output_data/\"\n",
    "\n",
    "train_df_processed = pd.read_csv(OUTPUT_DATA_DIR+\"text_processed_training.csv\")\n",
    "val_df_processed = pd.read_csv(OUTPUT_DATA_DIR+\"text_processed_validation.csv\")\n",
    "test_df_processed = pd.read_csv(OUTPUT_DATA_DIR+\"text_processed_testing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Features\n",
    "\n",
    "We now combine the features engineered on the raw data with the trained s values.\n",
    "\n",
    "Both datasets have interactions for combinations of user ids and book ids. However, the chainRec model expects the ids to be numberic. So we apply this same mapping to the raw data and then join the two datasets together to get our augmented set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping(mapping_file):\n",
    "    \"\"\"Loads the mapping from `mapping_file`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping_file: str\n",
    "        The name of the mapping file to import.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame created from the mapping.\n",
    "    \n",
    "    \"\"\"\n",
    "    return pd.read_csv(os.path.join(\"../mappings\", \"{}.csv\".format(mapping_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map = load_mapping(\"user_map\")\n",
    "book_map = load_mapping(\"book_map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_map['book_id'] = book_map['book_id'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_id(data_df, u_map, i_map):\n",
    "    \"\"\"Creates a user-item ID for the records in `data_df`.\n",
    "    \n",
    "    The user-item ID is created from `u_map` and `i_map`.\n",
    "    Both mappings, map text user IDs to numeric user IDs\n",
    "    and these numeric user IDs are combined to form the\n",
    "    user-item ID.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame for which the user-item ID is created.\n",
    "    u_map: pd.DataFrame\n",
    "        A DataFrame containing a mapping from a text user ID to\n",
    "        a numeric user ID.\n",
    "    i_map: pd.DataFrame\n",
    "        A DataFrame containing a mapping from a text item ID to\n",
    "        a numeric item ID.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after adding a\n",
    "        user-item ID field based on `u_map` and `i_map`.\n",
    "    \n",
    "    \"\"\"\n",
    "    data_df['book_id'] = data_df['book_id'].apply(lambda x: str(x))\n",
    "    data_df = pd.merge(data_df, u_map, how=\"left\", on=[\"user_id\"])\n",
    "    data_df = pd.merge(data_df, i_map, how=\"left\", on=[\"book_id\"])\n",
    "    data_df['user_number'] = data_df['user_number'].apply(lambda x: str(x))\n",
    "    data_df['book_number'] = data_df['book_number'].apply(lambda x: str(x))\n",
    "    data_df['user_item_id'] = data_df['user_number'] + \"-\" + data_df['book_number']\n",
    "    return data_df.drop(columns=['user_number', 'book_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_user_item_id(train_df_processed, user_map, book_map)\n",
    "val_df = create_user_item_id(val_df_processed, user_map, book_map)\n",
    "test_df = create_user_item_id(test_df_processed, user_map, book_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_values_df.drop(columns=['user_number', 'item_number'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_s = pd.merge(train_df, s_values_df, how='left', on=['user_item_id'])\n",
    "val_df_s = pd.merge(val_df, s_values_df, how='left', on=['user_item_id'])\n",
    "test_df_s = pd.merge(test_df, s_values_df, how='left', on=['user_item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_s.drop(columns=['user_item_id'], inplace=True)\n",
    "val_df_s.drop(columns=['user_item_id'], inplace=True)\n",
    "test_df_s.drop(columns=['user_item_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Augmented Data\n",
    "\n",
    "We save the augmented train, test, and validation datasets for easier access on future models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_s.to_csv(OUTPUT_DATA_DIR+\"training_s_vals_uniform.csv\", index=False)\n",
    "val_df_s.to_csv(OUTPUT_DATA_DIR+\"validation_s_vals_uniform.csv\", index=False)\n",
    "test_df_s.to_csv(OUTPUT_DATA_DIR+\"testing_s_vals_uniform.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Language Features\n",
    "\n",
    "We select the set of non language features which will be combined with the vectorized language features to get a full set of model features.\n",
    "\n",
    "We also scale the count features. As counts tend to grow exponentially we use a log transform to make the counts linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['text_reviews_count', 'is_ebook', 'average_rating', 'num_pages',\n",
    "                   'ratings_count', 'is_translated', 'is_in_series', 'series_length', \n",
    "                   'is_paperback', 'is_hardcover', 'is_audio', 'from_penguin', \n",
    "                   'from_harpercollins', 'from_university_press', 'from_vintage',\n",
    "                   'from_createspace', 'publication_year', 'author_a', 'author_b', \n",
    "                   'author_c', 'author_d', 'author_e', 'author_f', \n",
    "                   'book_shelved_count', 'shelved_count', 'read_count', 'rated_count', \n",
    "                   'recommended_count', 'title_len', 's1', 's2', 's3', 's4']\n",
    "X_train_reg = train_df_s[feature_columns]\n",
    "X_val_reg = val_df_s[feature_columns]\n",
    "X_test_reg = test_df_s[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_columns(data_df, cols):\n",
    "    \"\"\"Applies a log transform to `cols` in `data_df`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame in which the columns will be transformed.\n",
    "    cols: collection\n",
    "        The columns in `data_df` to be log scaled.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after log scaling\n",
    "        the columns `cols`.\n",
    "\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        data_df[col] = data_df[col].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-49ee2668773d>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[col] = data_df[col].apply(lambda x: np.log(x) if x > 0 else 0)\n"
     ]
    }
   ],
   "source": [
    "log_transform_cols = ['text_reviews_count', 'ratings_count', 'shelved_count', 'read_count', 'rated_count', 'recommended_count']\n",
    "X_train_reg = log_transform_columns(X_train_reg, log_transform_cols)\n",
    "X_val_reg = log_transform_columns(X_val_reg, log_transform_cols)\n",
    "X_test_reg = log_transform_columns(X_test_reg, log_transform_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Language Models\n",
    "\n",
    "We try some models that do not use any text based features first. Just by augmenting with the s values.\n",
    "\n",
    "##### Unscaled S Values\n",
    "\n",
    "First we use the raw s values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_reg1 = min_max_scaler.fit_transform(X_train_reg)\n",
    "X_val_reg1 = min_max_scaler.transform(X_val_reg)\n",
    "X_test_reg1 = min_max_scaler.transform(X_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7464768228906269\n",
      "Validation AUC: 0.6222168221329223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "reg_model = LogisticRegression(max_iter=200)\n",
    "reg_model.fit(X_train_reg1, train_df_s['recommended'])\n",
    "\n",
    "train_AUC = roc_auc_score(train_df_s['recommended'], reg_model.predict(X_train_reg1))\n",
    "val_AUC = roc_auc_score(val_df_s['recommended'], reg_model.predict(X_val_reg1))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7581636123023768\n",
      "Validation AUC: 0.6250423472961106\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg_cls = XGBClassifier(\n",
    "    objective='binary:logistic', learning_rate=0.1,\n",
    "    max_depth=1, n_estimators=1000)\n",
    "\n",
    "xg_cls.fit(X_train_reg1, train_df_processed['recommended'])\n",
    "train_AUC = roc_auc_score(\n",
    "    train_df_processed['recommended'], xg_cls.predict(X_train_reg1))\n",
    "val_AUC = roc_auc_score(\n",
    "    val_df_processed['recommended'], xg_cls.predict(X_val_reg1))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7662235855242499\n",
      "Validation AUC: 0.6193521992022147\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ranfor_model = RandomForestClassifier(n_estimators=1000, max_depth=10)\n",
    "ranfor_model.fit(X_train_reg1, train_df_processed['recommended'])\n",
    "\n",
    "train_AUC = roc_auc_score(\n",
    "    train_df_processed['recommended'], ranfor_model.predict(X_train_reg1))\n",
    "val_AUC = roc_auc_score(\n",
    "    val_df_processed['recommended'], ranfor_model.predict(X_val_reg1))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaled S Values\n",
    "\n",
    "Next we try the models after scaling the s values using a sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(val):\n",
    "    \"\"\"Applies the sigmoid function to `val`.\n",
    "    \n",
    "    The sigmoid function has the form\n",
    "    f(x) = 1 / (1 + exp(-x))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    val: float\n",
    "        The operand to the sigmoid function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The result of applying the sigmoid\n",
    "        function to `val`.\n",
    "    \n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_s_values(data_df):\n",
    "    \"\"\"Applies the sigmoid function to the s values in `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame for which the operation is performed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame that results from scaling the s values\n",
    "        in `data_df`.\n",
    "    \n",
    "    \"\"\"\n",
    "    for s_col in [\"s1\", \"s2\", \"s3\", \"s4\"]:\n",
    "        data_df[s_col] = data_df[s_col].apply(lambda x: sigmoid(x))\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-4768feb00e52>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[s_col] = data_df[s_col].apply(lambda x: sigmoid(x))\n",
      "<ipython-input-27-4768feb00e52>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[s_col] = data_df[s_col].apply(lambda x: sigmoid(x))\n",
      "<ipython-input-27-4768feb00e52>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[s_col] = data_df[s_col].apply(lambda x: sigmoid(x))\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_reg2 = min_max_scaler.fit_transform(scale_s_values(X_train_reg))\n",
    "X_val_reg2 = min_max_scaler.transform(scale_s_values(X_val_reg))\n",
    "X_test_reg2 = min_max_scaler.transform(scale_s_values(X_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7495256159396809\n",
      "Validation AUC: 0.6204607888926602\n"
     ]
    }
   ],
   "source": [
    "reg_model = LogisticRegression(max_iter=1000)\n",
    "reg_model.fit(X_train_reg2, train_df_s['recommended'])\n",
    "\n",
    "train_AUC = roc_auc_score(train_df_s['recommended'], reg_model.predict(X_train_reg2))\n",
    "val_AUC = roc_auc_score(val_df_s['recommended'], reg_model.predict(X_val_reg2))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7579780100571275\n",
      "Validation AUC: 0.6252612600969927\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg_cls = XGBClassifier(\n",
    "    objective='binary:logistic', learning_rate=0.1,\n",
    "    max_depth=1, n_estimators=1000)\n",
    "\n",
    "xg_cls.fit(X_train_reg2, train_df_processed['recommended'])\n",
    "train_AUC = roc_auc_score(\n",
    "    train_df_processed['recommended'], xg_cls.predict(X_train_reg2))\n",
    "val_AUC = roc_auc_score(\n",
    "    val_df_processed['recommended'], xg_cls.predict(X_val_reg2))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8184934152333979\n",
      "Validation AUC: 0.6243113901942385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ranfor_model = RandomForestClassifier(n_estimators=1000, max_depth=15)\n",
    "ranfor_model.fit(X_train_reg2, train_df_processed['recommended'])\n",
    "\n",
    "train_AUC = roc_auc_score(\n",
    "    train_df_processed['recommended'], ranfor_model.predict(X_train_reg2))\n",
    "val_AUC = roc_auc_score(\n",
    "    val_df_processed['recommended'], ranfor_model.predict(X_val_reg2))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "We create vector embeddings of the words in the book descriptions. Word2Vec captures the most important words and then the vectors for each important word in the book description are averaged to get a vector for the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = train_df_s[['book_id', 'cleaned_text']]\n",
    "book_df = book_df.drop_duplicates(subset=['book_id'])\n",
    "\n",
    "book_df['cleaned_text'] = book_df['cleaned_text'].apply(lambda x: \"\" if pd.isnull(x) else x)\n",
    "\n",
    "w2v = Word2Vec(list(book_df['cleaned_text']), size=200, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_book_vector(book_text, vec_length):\n",
    "    \"\"\"Creates a vector for the book given by `book_text`.\n",
    "\n",
    "    The word vectors for each word in `book_text` are\n",
    "    averaged to build a vector for the book.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    book_text: str\n",
    "        The book text for which the vector is generated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vector\n",
    "        A vector for the book.\n",
    "\n",
    "    \"\"\"\n",
    "    text_vecs = [word for word in str(book_text) if word in w2v.wv.vocab]\n",
    "    if len(text_vecs) > 0:\n",
    "        return np.mean(w2v[text_vecs], axis=0)\n",
    "    return np.zeros(vec_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-4533799a3578>:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  return np.mean(w2v[text_vecs], axis=0)\n"
     ]
    }
   ],
   "source": [
    "train_df_s['book_vector'] = train_df_s['cleaned_text'].apply(lambda x: create_book_vector(x, 200))\n",
    "val_df_s['book_vector'] = val_df_s['cleaned_text'].apply(lambda x: create_book_vector(x, 200))\n",
    "test_df_s['book_vector'] = test_df_s['cleaned_text'].apply(lambda x: create_book_vector(x, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_book_vec_df(book_vecs, indices):\n",
    "    \"\"\"Creates a dataframe from `book_vecs`.\n",
    "\n",
    "    Each numpy array in `book_vecs` is converted to a\n",
    "    row in the resulting dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    book_vecs: list\n",
    "        A list of numpy arrays where each array corresponds\n",
    "        to the book vector for a book.\n",
    "    indicies: np.array\n",
    "        A numpy array of indices for the DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from converting `review_vecs`\n",
    "        to a dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    book_vec_df = pd.DataFrame(np.vstack(book_vecs))\n",
    "    book_vec_df.columns = [\"word\" + str(col) for col in book_vec_df.columns]\n",
    "    book_vec_df.index = indices\n",
    "    return book_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wv = create_book_vec_df(train_df_s['book_vector'], train_df_s.index)\n",
    "val_wv = create_book_vec_df(val_df_s['book_vector'], val_df_s.index)\n",
    "test_wv = create_book_vec_df(test_df_s['book_vector'], test_df_s.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wv_reg = pd.concat([train_wv, pd.DataFrame(np.vstack(X_train_reg2))], axis=1)\n",
    "X_val_wv_reg = pd.concat([val_wv, pd.DataFrame(np.vstack(X_val_reg2))], axis=1)\n",
    "X_test_wv_reg = pd.concat([test_wv, pd.DataFrame(np.vstack(X_test_reg2))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7500566573755848\n",
      "Validation AUC: 0.6210230941261254\n"
     ]
    }
   ],
   "source": [
    "reg_model = LogisticRegression(max_iter=1000)\n",
    "reg_model.fit(X_train_wv_reg, train_df_s['recommended'])\n",
    "\n",
    "train_AUC = roc_auc_score(train_df_s['recommended'], reg_model.predict(X_train_wv_reg))\n",
    "val_AUC = roc_auc_score(val_df_s['recommended'], reg_model.predict(X_val_wv_reg))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-4768feb00e52>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[s_col] = data_df[s_col].apply(lambda x: sigmoid(x))\n"
     ]
    }
   ],
   "source": [
    "X_train_reg_s = scale_s_values(X_train_reg)\n",
    "X_val_reg_s = scale_s_values(X_val_reg)\n",
    "X_test_reg_s = scale_s_values(X_test_reg)\n",
    "\n",
    "X_train_wv_reg1 = pd.concat([train_wv, X_train_reg_s], axis=1)\n",
    "X_val_wv_reg1 = pd.concat([val_wv, X_val_reg_s], axis=1)\n",
    "X_test_wv_reg1 = pd.concat([test_wv, X_test_reg_s], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.7616114497718668\n",
      "Validation AUC: 0.6228888673499191\n"
     ]
    }
   ],
   "source": [
    "xg_cls = XGBClassifier(\n",
    "    objective='binary:logistic', learning_rate=0.1,\n",
    "    max_depth=1, n_estimators=2000)\n",
    "\n",
    "xg_cls.fit(X_train_wv_reg1, train_df_processed['recommended'])\n",
    "train_AUC = roc_auc_score(\n",
    "    train_df_processed['recommended'], xg_cls.predict(X_train_wv_reg1))\n",
    "val_AUC = roc_auc_score(\n",
    "    val_df_processed['recommended'], xg_cls.predict(X_val_wv_reg1))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8147323884547026\n",
      "Validation AUC: 0.6217541046083439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ranfor_model = RandomForestClassifier(n_estimators=1000, max_depth=15)\n",
    "ranfor_model.fit(X_train_wv_reg, train_df_processed['recommended'])\n",
    "\n",
    "train_AUC = roc_auc_score(\n",
    "    train_df_processed['recommended'], ranfor_model.predict(X_train_wv_reg))\n",
    "val_AUC = roc_auc_score(\n",
    "    val_df_processed['recommended'], ranfor_model.predict(X_val_wv_reg))\n",
    "\n",
    "print(\"Training AUC: {}\".format(train_AUC))\n",
    "print(\"Validation AUC: {}\".format(val_AUC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
