{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Baseline Models\n",
    "\n",
    "These language models serve as a baseline and will be leverage the book title and description in order to hopefully enhance the predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DATA_DIR = \"./output_data/\"\n",
    "\n",
    "train_df = pd.read_csv(OUTPUT_DATA_DIR+\"interactions_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(OUTPUT_DATA_DIR+\"interactions_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data For Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data For Textual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_book_text(book_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    book_text: str\n",
    "        The book text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "    \n",
    "    \"\"\"\n",
    "    book = re.sub('[^a-zA-Z0-9]', ' ', book_text).lower().split()\n",
    "    book = [ps.stem(word) for word in book if not word in exclude_text]\n",
    "    return ' '.join(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_classification(data_df):\n",
    "    \"\"\"Preprocesses `data_df` to be used in classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after processing.\n",
    "    \n",
    "    \"\"\"\n",
    "    # flags for most popular formats\n",
    "    data_df['format'] = data_df['format'].apply(lambda x: str(x).lower())\n",
    "    data_df['is_paperback'] = data_df['format'].apply(lambda x: int(\"paper\" in x))\n",
    "    data_df['is_hardcover'] = data_df['format'].apply(lambda x: int(\"hard\" in x))\n",
    "    data_df['is_audio'] = data_df['format'].apply(lambda x: int(\"audio\" in x))\n",
    "    data_df['is_other_format'] = (data_df['is_paperback'] + data_df['is_hardcover'] + \n",
    "                                  data_df['is_audio'] + data_df['is_ebook'])\n",
    "    data_df['is_other_format'] = data_df['is_other_format'].apply(lambda x: 0 if x > 0 else 1)\n",
    "    \n",
    "    #flags for most popular publishers\n",
    "    data_df['publisher'] = data_df['publisher'].apply(lambda x: str(x).lower())\n",
    "    data_df['from_penguin'] = data_df['publisher'].apply(lambda x: int(\"penguin\" in x))\n",
    "    data_df['from_harpercollins'] = data_df['publisher'].apply(lambda x: int(\"harpercollins\" in x or \"harper collins\" in x))\n",
    "    data_df['from_university_press'] = data_df['publisher'].apply(lambda x: int(\"university press\" in x))\n",
    "    data_df['from_vintage'] = data_df['publisher'].apply(lambda x: int(\"vintage\" in x))\n",
    "    data_df['from_createspace'] = data_df['publisher'].apply(lambda x: int(\"createspace\" in x or \"create space\" in x))\n",
    "    data_df['other_publisher'] = (data_df['from_penguin'] + data_df['from_harpercollins'] + \n",
    "                                  data_df['from_university_press'] + data_df['from_vintage'] + data_df['from_createspace'])\n",
    "    data_df['other_publisher'] = data_df['other_publisher'].apply(lambda x: 0 if x > 0 else 1)\n",
    "    \n",
    "    # ensuring columns are not missing\n",
    "    train_df['average_rating'] = train_df['average_rating'].apply(lambda x: 0.0 if pd.isnull(x) else x)\n",
    "    train_df['text_reviews_count'] = train_df['text_reviews_count'].apply(lambda x: 0 if pd.isnull(x) else x)\n",
    "    train_df['ratings_count'] = train_df['ratings_count'].apply(lambda x: 0 if pd.isnull(x) else x)\n",
    "    \n",
    "    median_page_count = train_df['num_pages'].median()\n",
    "    train_df['num_pages'] = train_df['num_pages'].apply(lambda x: median_page_count if pd.isnull(x) else x)\n",
    "    \n",
    "    # flags for most popular authors\n",
    "    train_df['main_author'] = train_df['main_author'].astype(str)\n",
    "    train_df['author_a'] = train_df['main_author'].apply(lambda x: int(x == \"435477.0\"))\n",
    "    train_df['author_b'] = train_df['main_author'].apply(lambda x: int(x == \"903.0\"))\n",
    "    train_df['author_c'] = train_df['main_author'].apply(lambda x: int(x == \"947.0\"))\n",
    "    train_df['author_d'] = train_df['main_author'].apply(lambda x: int(x == \"4624490.0\"))\n",
    "    train_df['author_e'] = train_df['main_author'].apply(lambda x: int(x == \"18540.0\"))\n",
    "    train_df['author_f'] = train_df['main_author'].apply(lambda x: int(x == \"8075577.0\"))\n",
    "    train_df['author_other'] = (train_df['author_a'] + train_df['author_b'] + \n",
    "                                train_df['author_c'] + train_df['author_d'] + \n",
    "                                train_df['author_e'] +train_df['author_f'])\n",
    "    train_df['author_other'] = train_df['author_other'].apply(lambda x: 0 if x > 0 else 1)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_book_text(data_df, id_col, text_col, exclude_text, ps):\n",
    "    \"\"\"Preprocesses the book text in `data_df` for `text_col`.\n",
    "    \n",
    "    The dataframe is restricted to `id_col` and `text_col` and then the\n",
    "    unique ids are chosen. This is so that we only preprocess the text\n",
    "    for a book once. Then we join the resulting text back to `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame containing the data to be preprocessed.\n",
    "    id_col: str\n",
    "        The column from which unique ids are chosen.\n",
    "    text_col: str\n",
    "        The column to be pre-processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to remove\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used for word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after adding a column\n",
    "        with the processed text.\n",
    "    \n",
    "    \"\"\"\n",
    "    book_df = train_df[[id_col, text_col]]\n",
    "    book_df = book_df.drop_duplicates(subset=[id_col])\n",
    "    book_df['cleaned_text'] = book_df[text_col].apply(lambda x: process_book_text(x, exclude_text, ps))\n",
    "    final_df = pd.merge(train_df, book_df[[id_col, \"cleaned_text\"]], how=\"inner\", on=[id_col])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocess_pipeline(data_df, exclude_text, ps):\n",
    "    \"\"\"Runs the full pre-processing pipeline on `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    processed_df = preprocess_for_classification(data_df)\n",
    "    return preprocess_all_book_text(processed_df, \"book_id\", \"title_description\", exclude_text, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "format\n",
       "Hardcover and Paperback                           1\n",
       "Paperback, E-book                                 1\n",
       "Paper, Color Photo and Black Twine                1\n",
       "Trade paperback                                   1\n",
       "Turtleback                                        1\n",
       "cards in envelope                                 1\n",
       "Letterpressed Chapbook                            1\n",
       "chapbook/ebook                                    1\n",
       "Hardcover/Paperback                               1\n",
       "hand-stitched chapbook                            1\n",
       "Hardcover first edition                           1\n",
       "Poetry chapbook                                   1\n",
       "saddle stitched chapbook                          1\n",
       "Handmade Chapbook                                 1\n",
       "Chapbook/Pamphlet                                 1\n",
       "perfect bound                                     1\n",
       "print                                             1\n",
       "Board book                                        1\n",
       "japanese stab bound                               1\n",
       "hardback                                          2\n",
       "Audio                                             2\n",
       "Slipcased Hardcover                               2\n",
       "Board Book                                        2\n",
       "chapbook/e-chap                                   2\n",
       "Casebound                                         2\n",
       "newsprint stapled folio                           2\n",
       "poetry chapbook                                   2\n",
       "Perfect Bound                                     2\n",
       "Softcover                                         3\n",
       "ribbon-bound chapbook                             3\n",
       "saddle-stitched chapbook                          3\n",
       "Paper and Photographic Cardboard                  3\n",
       "Hardback                                          3\n",
       "Library Binding                                   4\n",
       "Publisher's Binding                               4\n",
       "e-chapbook                                        4\n",
       "PDF (Movie Script)                                4\n",
       "cloth                                             6\n",
       "Paperback, kindle &amp; ebook.                    7\n",
       "Unbound                                           8\n",
       "Audio Cassette                                    8\n",
       "Handmade chapbook                                 8\n",
       "hardcover                                         8\n",
       "Poem Cards                                        9\n",
       "chapbook                                         10\n",
       "html                                             12\n",
       "Audiobook                                        12\n",
       "Hardcover, Sewn Binding, Paper Dust Jacket       16\n",
       "Audible Audio                                    23\n",
       "paperback                                        24\n",
       "Chapbook                                         29\n",
       "Audio CD                                         29\n",
       "Unknown Binding                                  44\n",
       "Leather Bound                                    66\n",
       "paper                                           201\n",
       "Mass Market Paperback                           208\n",
       "Kindle Edition                                  313\n",
       "ebook                                           313\n",
       "Hardcover                                     13123\n",
       "Paperback                                     35695\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_counts = train_df.groupby(train_df['format'])['user_id'].count()\n",
    "format_counts.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publisher\n",
       "Mariner Books                     517\n",
       "Simon & Schuster                  520\n",
       "City Lights                       528\n",
       "Graywolf Press                    564\n",
       "HarperCollins                     566\n",
       "CreateSpace                       665\n",
       "Penguin Books                     699\n",
       "Penguin                           718\n",
       "Rupa & Co                         743\n",
       "Everyman's Library                756\n",
       "New Directions                    802\n",
       "Oxford University Press           821\n",
       "Doubleday & Company, Inc.         858\n",
       "Andrews McMeel Publishing         905\n",
       "Createspace                       921\n",
       "Dover Publications                938\n",
       "Vintage                          1018\n",
       "Cambridge University Press       2442\n",
       "Harpercollins Childrens Books    4448\n",
       "Penguin Classics                 5920\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_counts = train_df.groupby(train_df['publisher'])['user_id'].count()\n",
    "publisher_counts = publisher_counts.sort_values()\n",
    "publisher_counts[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "2642\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "203.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_df[pd.isnull(train_df['average_rating'])]))\n",
    "print(len(train_df[pd.isnull(train_df['text_reviews_count'])]))\n",
    "print(len(train_df[pd.isnull(train_df['num_pages'])]))\n",
    "print(len(train_df[pd.isnull(train_df['ratings_count'])]))\n",
    "train_df['num_pages'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 824  862  952  972 1104 1283 1814 3626 4118 4528]\n",
      "Float64Index([8075577.0, 18540.0, 4624490.0, 947.0, 903.0, 435477.0], dtype='float64', name='main_author')\n"
     ]
    }
   ],
   "source": [
    "author_counts = train_df.groupby(train_df['main_author'])['user_id'].count()\n",
    "author_counts = author_counts.sort_values()\n",
    "print(author_counts.values[-10:])\n",
    "print(author_counts.index[-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>title_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113914</th>\n",
       "      <td>18295863</td>\n",
       "      <td>Letter Composed During a Lull in the Fighting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224359</th>\n",
       "      <td>18003300</td>\n",
       "      <td>Love &amp; Misadventure Lang Leav is a poet and in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90635</th>\n",
       "      <td>46199</td>\n",
       "      <td>Letters to a Young Poet In 1903, a student at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113523</th>\n",
       "      <td>30119</td>\n",
       "      <td>Where the Sidewalk Ends Where the Sidewalk End...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196420</th>\n",
       "      <td>5289</td>\n",
       "      <td>Complete Works of Oscar Wilde In print since 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38204</th>\n",
       "      <td>35498776</td>\n",
       "      <td>The Sky Threw Stars the storm. the strike. the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172382</th>\n",
       "      <td>874604</td>\n",
       "      <td>Collected Verse of Edgar A. Guest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173754</th>\n",
       "      <td>31122069</td>\n",
       "      <td>The Lay of Aotrou and Itroun The Lay of Aotrou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67472</th>\n",
       "      <td>22736736</td>\n",
       "      <td>September First</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208711</th>\n",
       "      <td>1058137</td>\n",
       "      <td>Selected Poems: Keats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4630 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         book_id                                  title_description\n",
       "113914  18295863  Letter Composed During a Lull in the Fighting ...\n",
       "224359  18003300  Love & Misadventure Lang Leav is a poet and in...\n",
       "90635      46199  Letters to a Young Poet In 1903, a student at ...\n",
       "113523     30119  Where the Sidewalk Ends Where the Sidewalk End...\n",
       "196420      5289  Complete Works of Oscar Wilde In print since 1...\n",
       "...          ...                                                ...\n",
       "38204   35498776  The Sky Threw Stars the storm. the strike. the...\n",
       "172382    874604                 Collected Verse of Edgar A. Guest \n",
       "173754  31122069  The Lay of Aotrou and Itroun The Lay of Aotrou...\n",
       "67472   22736736                                   September First \n",
       "208711   1058137                             Selected Poems: Keats \n",
       "\n",
       "[4630 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df = train_df[['book_id', 'title_description']]\n",
    "book_df = book_df.drop_duplicates(subset=['book_id'])\n",
    "book_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "train_df_processed = run_preprocess_pipeline(train_df, exclude_english, ps)\n",
    "val_df_processed = run_preprocess_pipeline(val_df, exclude_english, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_dataset(data_df):\n",
    "    \"\"\"Randomly shuffles `df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The DataFrame to be shuffled.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A shuffled dataframe obtained from `df`.\n",
    "    \n",
    "    \"\"\"\n",
    "    data_df = shuffle(data_df)\n",
    "    data_df.reset_index(inplace=True, drop=True)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshuffle the datasets as they were sorted for the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_processed = shuffle_dataset(train_df_processed)\n",
    "val_df_processed = shuffle_dataset(val_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_processed.to_csv(OUTPUT_DATA_DIR+\"text_processed_training.csv\")\n",
    "val_df_processed.to_csv(OUTPUT_DATA_DIR+\"text_processed_validation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a set of non text features which we will add to our textual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['text_reviews_count', 'is_ebook', 'average_rating', 'num_pages', \n",
    "                   'publication_year', 'ratings_count', 'is_translated', 'is_in_series',\n",
    "                   'series_length', 'is_paperback', 'is_hardcover', 'is_audio', 'is_other_format',\n",
    "                   'from_penguin', 'from_harpercollins', 'from_university_press', 'from_vintage',\n",
    "                   'from_createspace', 'other_publisher', 'author_a', 'author_b', 'author_c',\n",
    "                   'author_d', 'author_e', 'author_f', 'author_other']\n",
    "X_train_reg = train_df_processed[columns_to_keep]\n",
    "X_val_reg = val_df_processed[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_columns(data_df, cols):\n",
    "    \"\"\"Applies a log transform to `cols` in `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame in which the columns will be transformed.\n",
    "    cols: collection\n",
    "        The columns in `data_df` to be log scaled.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after log scaling\n",
    "        the columns `cols`.\n",
    "    \n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        data_df[col] = data_df[col].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-88fb59fe720e>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[col] = data_df[col].apply(lambda x: np.log(x) if x > 0 else 0)\n"
     ]
    }
   ],
   "source": [
    "log_transform_cols = ['text_reviews_count', 'ratings_count']\n",
    "X_train_reg = log_transform_columns(X_train_reg, log_transform_cols)\n",
    "X_val_reg = log_transform_columns(X_val_reg, log_transform_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_reg = min_max_scaler.fit_transform(X_train_reg)\n",
    "X_val_reg = min_max_scaler.transform(X_val_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Textual Model\n",
    "\n",
    "We apply TF-IDF only to the unique corpus of book descriptions. We only want to apply it to the unique descriptions because we do not want to overweight the books that are frequently read.\n",
    "\n",
    "We start by getting the book corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = train_df_processed[['book_id', 'cleaned_text']]\n",
    "book_df = book_df.drop_duplicates(subset=['book_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we fit TF-IDF to the book corpus and then use it to transform the training and validation text to a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "\n",
    "tfidf_model.fit(book_df['cleaned_text'])\n",
    "\n",
    "train_tfidf = tfidf_model.transform(train_df_processed['cleaned_text'])\n",
    "val_tfidf = tfidf_model.transform(val_df_processed['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the language features with the remaining features. As the language features are already in the form of a sparse matrix we convert the remaining_features to a sparse matrix as a sparse matrix will be more efficient than a dataframe for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_reg_sp = sp.csr_matrix(X_train_reg)\n",
    "X_train_tfidf_reg = sp.hstack((train_tfidf, X_train_reg_sp), format='csr')\n",
    "\n",
    "X_val_reg_sp = sp.csr_matrix(X_val_reg)\n",
    "X_val_tfidf_reg = sp.hstack((val_tfidf, X_val_reg_sp), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.03, # Estimators: 50, Depth: 1\n",
      "---------------------------------------------------\n",
      "Training AUC: 0.6519365968382808\n",
      "Validation AUC: 0.6519365968382808\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 2\n",
      "---------------------------------------------------\n",
      "Training AUC: 0.6532192151104823\n",
      "Validation AUC: 0.6532192151104823\n",
      "\n",
      "Learning Rate: 0.03, # Estimators: 50, Depth: 5\n",
      "---------------------------------------------------\n",
      "Training AUC: 0.6544425647098107\n",
      "Validation AUC: 0.6544425647098107\n",
      "\n",
      "Learning Rate: 0.05, # Estimators: 50, Depth: 1\n",
      "---------------------------------------------------\n",
      "Training AUC: 0.6519365968382808\n",
      "Validation AUC: 0.6519365968382808\n",
      "\n",
      "Learning Rate: 0.05, # Estimators: 50, Depth: 2\n",
      "---------------------------------------------------\n",
      "Training AUC: 0.6536800774496418\n",
      "Validation AUC: 0.6536800774496418\n",
      "\n",
      "Learning Rate: 0.05, # Estimators: 50, Depth: 5\n",
      "---------------------------------------------------\n",
      "Training AUC: 0.6549579909757283\n",
      "Validation AUC: 0.6549579909757283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "learning_rates = [0.03, 0.05]\n",
    "estimators = [50]\n",
    "depths = [1, 2, 5]\n",
    "\n",
    "lrs = []\n",
    "estim = []\n",
    "ds = []\n",
    "train_MSEs = []\n",
    "val_MSEs = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for estimator in estimators:\n",
    "        for depth in depths:\n",
    "            lrs.append(learning_rate)\n",
    "            estim.append(estimator)\n",
    "            ds.append(depth)\n",
    "            print(\"Learning Rate: {0}, # Estimators: {1}, Depth: {2}\".format(learning_rate, estimator, depth))\n",
    "            print(\"---------------------------------------------------\")\n",
    "            xg_cls = XGBClassifier(\n",
    "                objective='binary:logistic', learning_rate=learning_rate,\n",
    "                max_depth=depth, n_estimators=estimator)\n",
    "            xg_cls.fit(X_train_tfidf_reg, train_df_processed['recommended'])\n",
    "            train_MSEs.append(roc_auc_score(\n",
    "                train_df_processed['recommended'], xg_cls.predict(X_train_tfidf_reg)))\n",
    "            val_MSEs.append(roc_auc_score(\n",
    "                val_df_processed['recommended'], xg_cls.predict(X_val_tfidf_reg)))\n",
    "            print(\"Training AUC: {}\".format(train_MSEs[-1]))\n",
    "            print(\"Validation AUC: {}\".format(val_MSEs[-1]))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = './results/'\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tfidf = pd.DataFrame({'learning_rate': lrs,\n",
    "                          'n_estimators': estim,\n",
    "                          'max_depth': ds,\n",
    "                          'training_MSE': train_MSEs,\n",
    "                          'validation_MSE': val_MSEs})\n",
    "xgb_tfidf.to_csv(RESULTS_DIR+\"xgbTFIDF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.6879847257109832\n",
      "Validation AUC: 0.6879847257109832\n"
     ]
    }
   ],
   "source": [
    "xg_cls = XGBClassifier(\n",
    "    objective='binary:logistic', learning_rate=0.3, max_depth=2, n_estimators=2000)\n",
    "\n",
    "xg_cls.fit(X_train_tfidf_reg, train_df_processed['recommended'])\n",
    "\n",
    "train_MSE = roc_auc_score(\n",
    "                train_df_processed['recommended'], xg_cls.predict(X_train_tfidf_reg))\n",
    "val_MSE = roc_auc_score(\n",
    "                val_df_processed['recommended'], xg_cls.predict(X_val_tfidf_reg))\n",
    "print(\"Training AUC: {}\".format(train_MSE))\n",
    "print(\"Validation AUC: {}\".format(val_MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
