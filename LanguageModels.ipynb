{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Baseline Models\n",
    "\n",
    "These language models serve as a baseline and will be leverage the book title and description in order to hopefully enhance the predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DATA_DIR = \"./output_data/\"\n",
    "\n",
    "train_df = pd.read_csv(OUTPUT_DATA_DIR+\"interactions_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(OUTPUT_DATA_DIR+\"interactions_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data For Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data For Textual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_book_text(book_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    book_text: str\n",
    "        The book text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "    \n",
    "    \"\"\"\n",
    "    book = re.sub('[^a-zA-Z0-9]', ' ', book_text).lower().split()\n",
    "    book = [ps.stem(word) for word in book if not word in exclude_text]\n",
    "    return ' '.join(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_classification(data_df):\n",
    "    \"\"\"Preprocesses `data_df` to be used in classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame to be processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after processing.\n",
    "    \n",
    "    \"\"\"\n",
    "    # flags for most popular formats\n",
    "    data_df['format'] = data_df['format'].apply(lambda x: str(x).lower())\n",
    "    data_df['is_paperback'] = data_df['format'].apply(lambda x: int(\"paper\" in x))\n",
    "    data_df['is_hardcover'] = data_df['format'].apply(lambda x: int(\"hard\" in x))\n",
    "    data_df['is_audio'] = data_df['format'].apply(lambda x: int(\"audio\" in x))\n",
    "    data_df['is_other_format'] = (data_df['is_paperback'] + data_df['is_hardcover'] + \n",
    "                                  data_df['is_audio'] + data_df['is_ebook'])\n",
    "    data_df['is_other_format'] = data_df['is_other_format'].apply(lambda x: 0 if x > 0 else 1)\n",
    "    \n",
    "    #flags for most popular publishers\n",
    "    data_df['publisher'] = data_df['publisher'].apply(lambda x: str(x).lower())\n",
    "    data_df['from_penguin'] = data_df['publisher'].apply(lambda x: int(\"penguin\" in x))\n",
    "    data_df['from_harpercollins'] = data_df['publisher'].apply(lambda x: int(\"harpercollins\" in x or \"harper collins\" in x))\n",
    "    data_df['from_university_press'] = data_df['publisher'].apply(lambda x: int(\"university press\" in x))\n",
    "    data_df['from_vintage'] = data_df['publisher'].apply(lambda x: int(\"vintage\" in x))\n",
    "    data_df['from_createspace'] = data_df['publisher'].apply(lambda x: int(\"createspace\" in x or \"create space\" in x))\n",
    "    data_df['other_publisher'] = (data_df['from_penguin'] + data_df['from_harpercollins'] + \n",
    "                                  data_df['from_university_press'] + data_df['from_vintage'] + data_df['from_createspace'])\n",
    "    data_df['other_publisher'] = data_df['other_publisher'].apply(lambda x: 0 if x > 0 else 1)\n",
    "    \n",
    "    # ensuring columns are not missing\n",
    "    train_df['average_rating'] = train_df['average_rating'].apply(lambda x: 0.0 if pd.isnull(x) else x)\n",
    "    train_df['text_reviews_count'] = train_df['text_reviews_count'].apply(lambda x: 0 if pd.isnull(x) else x)\n",
    "    train_df['ratings_count'] = train_df['ratings_count'].apply(lambda x: 0 if pd.isnull(x) else x)\n",
    "    \n",
    "    median_page_count = train_df['num_pages'].median()\n",
    "    train_df['num_pages'] = train_df['num_pages'].apply(lambda x: median_page_count if pd.isnull(x) else x)\n",
    "    \n",
    "    # flags for most popular authors\n",
    "    train_df['main_author'] = train_df['main_author'].astype(str)\n",
    "    train_df['author_a'] = train_df['main_author'].apply(lambda x: int(x == \"435477.0\"))\n",
    "    train_df['author_b'] = train_df['main_author'].apply(lambda x: int(x == \"903.0\"))\n",
    "    train_df['author_c'] = train_df['main_author'].apply(lambda x: int(x == \"947.0\"))\n",
    "    train_df['author_d'] = train_df['main_author'].apply(lambda x: int(x == \"4624490.0\"))\n",
    "    train_df['author_e'] = train_df['main_author'].apply(lambda x: int(x == \"18540.0\"))\n",
    "    train_df['author_f'] = train_df['main_author'].apply(lambda x: int(x == \"8075577.0\"))\n",
    "    train_df['author_other'] = (train_df['author_a'] + train_df['author_b'] + \n",
    "                                train_df['author_c'] + train_df['author_d'] + \n",
    "                                train_df['author_e'] +train_df['author_f'])\n",
    "    train_df['author_other'] = train_df['author_other'].apply(lambda x: 0 if x > 0 else 1)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_book_text(data_df, id_col, text_col, exclude_text, ps):\n",
    "    \"\"\"Preprocesses the book text in `data_df` for `text_col`.\n",
    "    \n",
    "    The dataframe is restricted to `id_col` and `text_col` and then the\n",
    "    unique ids are chosen. This is so that we only preprocess the text\n",
    "    for a book once. Then we join the resulting text back to `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame containing the data to be preprocessed.\n",
    "    id_col: str\n",
    "        The column from which unique ids are chosen.\n",
    "    text_col: str\n",
    "        The column to be pre-processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to remove\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used for word stemming.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after adding a column\n",
    "        with the processed text.\n",
    "    \n",
    "    \"\"\"\n",
    "    book_df = train_df[[id_col, text_col]]\n",
    "    book_df = book_df.drop_duplicates(subset=[id_col])\n",
    "    book_df['cleaned_text'] = book_df[text_col].apply(lambda x: process_book_text(x, exclude_text, ps))\n",
    "    final_df = pd.merge(train_df, book_df[[id_col, \"cleaned_text\"]], how=\"inner\", on=[id_col])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocess_pipeline(data_df, exclude_text, ps):\n",
    "    \"\"\"Runs the full pre-processing pipeline on `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    processed_df = preprocess_for_classification(data_df)\n",
    "    return preprocess_all_book_text(processed_df, \"book_id\", \"title_description\", exclude_text, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "format\n",
       "hardcover and paperback                           1\n",
       "print                                             1\n",
       "hardcover first edition                           1\n",
       "trade paperback                                   1\n",
       "letterpressed chapbook                            1\n",
       "hand-stitched chapbook                            1\n",
       "turtleback                                        1\n",
       "hardcover/paperback                               1\n",
       "chapbook/ebook                                    1\n",
       "chapbook/pamphlet                                 1\n",
       "saddle stitched chapbook                          1\n",
       "cards in envelope                                 1\n",
       "paper, color photo and black twine                1\n",
       "paperback, e-book                                 1\n",
       "japanese stab bound                               1\n",
       "casebound                                         2\n",
       "newsprint stapled folio                           2\n",
       "slipcased hardcover                               2\n",
       "audio                                             2\n",
       "chapbook/e-chap                                   2\n",
       "saddle-stitched chapbook                          3\n",
       "paper and photographic cardboard                  3\n",
       "ribbon-bound chapbook                             3\n",
       "softcover                                         3\n",
       "perfect bound                                     3\n",
       "board book                                        3\n",
       "poetry chapbook                                   3\n",
       "pdf (movie script)                                4\n",
       "library binding                                   4\n",
       "e-chapbook                                        4\n",
       "publisher's binding                               4\n",
       "hardback                                          5\n",
       "cloth                                             6\n",
       "paperback, kindle &amp; ebook.                    7\n",
       "unbound                                           8\n",
       "audio cassette                                    8\n",
       "handmade chapbook                                 9\n",
       "poem cards                                        9\n",
       "audiobook                                        12\n",
       "html                                             12\n",
       "hardcover, sewn binding, paper dust jacket       16\n",
       "audible audio                                    23\n",
       "audio cd                                         29\n",
       "chapbook                                         39\n",
       "unknown binding                                  44\n",
       "leather bound                                    66\n",
       "paper                                           201\n",
       "mass market paperback                           208\n",
       "kindle edition                                  313\n",
       "ebook                                           313\n",
       "nan                                            6968\n",
       "hardcover                                     13131\n",
       "paperback                                     35719\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_counts = train_df.groupby(train_df['format'])['user_id'].count()\n",
    "format_counts.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publisher\n",
       "mariner books                     517\n",
       "simon & schuster                  520\n",
       "city lights                       528\n",
       "graywolf press                    564\n",
       "harpercollins                     566\n",
       "penguin books                     699\n",
       "penguin                           718\n",
       "rupa & co                         743\n",
       "everyman's library                756\n",
       "new directions                    802\n",
       "oxford university press           821\n",
       "doubleday & company, inc.         858\n",
       "andrews mcmeel publishing         905\n",
       "dover publications                938\n",
       "vintage                          1018\n",
       "createspace                      1586\n",
       "cambridge university press       2442\n",
       "harpercollins childrens books    4448\n",
       "nan                              5536\n",
       "penguin classics                 5920\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publisher_counts = train_df.groupby(train_df['publisher'])['user_id'].count()\n",
    "publisher_counts = publisher_counts.sort_values()\n",
    "publisher_counts[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "203.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_df[pd.isnull(train_df['average_rating'])]))\n",
    "print(len(train_df[pd.isnull(train_df['text_reviews_count'])]))\n",
    "print(len(train_df[pd.isnull(train_df['num_pages'])]))\n",
    "print(len(train_df[pd.isnull(train_df['ratings_count'])]))\n",
    "train_df['num_pages'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 862  952  972 1104 1169 1283 1814 3626 4118 4528]\n",
      "Index(['nan', '18540.0', '4624490.0', '947.0', '903.0', '435477.0'], dtype='object', name='main_author')\n"
     ]
    }
   ],
   "source": [
    "author_counts = train_df.groupby(train_df['main_author'])['user_id'].count()\n",
    "author_counts = author_counts.sort_values()\n",
    "print(author_counts.values[-10:])\n",
    "print(author_counts.index[-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>title_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113914</th>\n",
       "      <td>18295863</td>\n",
       "      <td>Letter Composed During a Lull in the Fighting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224359</th>\n",
       "      <td>18003300</td>\n",
       "      <td>Love &amp; Misadventure Lang Leav is a poet and in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90635</th>\n",
       "      <td>46199</td>\n",
       "      <td>Letters to a Young Poet In 1903, a student at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113523</th>\n",
       "      <td>30119</td>\n",
       "      <td>Where the Sidewalk Ends Where the Sidewalk End...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196420</th>\n",
       "      <td>5289</td>\n",
       "      <td>Complete Works of Oscar Wilde In print since 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38204</th>\n",
       "      <td>35498776</td>\n",
       "      <td>The Sky Threw Stars the storm. the strike. the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172382</th>\n",
       "      <td>874604</td>\n",
       "      <td>Collected Verse of Edgar A. Guest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173754</th>\n",
       "      <td>31122069</td>\n",
       "      <td>The Lay of Aotrou and Itroun The Lay of Aotrou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67472</th>\n",
       "      <td>22736736</td>\n",
       "      <td>September First</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208711</th>\n",
       "      <td>1058137</td>\n",
       "      <td>Selected Poems: Keats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4630 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         book_id                                  title_description\n",
       "113914  18295863  Letter Composed During a Lull in the Fighting ...\n",
       "224359  18003300  Love & Misadventure Lang Leav is a poet and in...\n",
       "90635      46199  Letters to a Young Poet In 1903, a student at ...\n",
       "113523     30119  Where the Sidewalk Ends Where the Sidewalk End...\n",
       "196420      5289  Complete Works of Oscar Wilde In print since 1...\n",
       "...          ...                                                ...\n",
       "38204   35498776  The Sky Threw Stars the storm. the strike. the...\n",
       "172382    874604                 Collected Verse of Edgar A. Guest \n",
       "173754  31122069  The Lay of Aotrou and Itroun The Lay of Aotrou...\n",
       "67472   22736736                                   September First \n",
       "208711   1058137                             Selected Poems: Keats \n",
       "\n",
       "[4630 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df = train_df[['book_id', 'title_description']]\n",
    "book_df = book_df.drop_duplicates(subset=['book_id'])\n",
    "book_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "train_df_processed = run_preprocess_pipeline(train_df, exclude_english, ps)\n",
    "val_df_processed = run_preprocess_pipeline(val_df, exclude_english, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_dataset(data_df):\n",
    "    \"\"\"Randomly shuffles `df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The DataFrame to be shuffled.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A shuffled dataframe obtained from `df`.\n",
    "    \n",
    "    \"\"\"\n",
    "    data_df = shuffle(data_df)\n",
    "    data_df.reset_index(inplace=True, drop=True)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshuffle the datasets as they were sorted for the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_processed = shuffle_dataset(train_df_processed)\n",
    "val_df_processed = shuffle_dataset(val_df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a set of non text features which we will add to our textual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['text_reviews_count', 'is_ebook', 'average_rating', 'num_pages', \n",
    "                   'publication_year', 'ratings_count', 'is_translated', 'is_in_series',\n",
    "                   'series_length', 'is_paperback', 'is_hardcover', 'is_audio', 'is_other_format',\n",
    "                   'from_penguin', 'from_harpercollins', 'from_university_press', 'from_vintage',\n",
    "                   'from_createspace', 'other_publisher', 'author_a', 'author_b', 'author_c',\n",
    "                   'author_d', 'author_e', 'author_f', 'author_other']\n",
    "X_train_reg = train_df_processed[columns_to_keep]\n",
    "X_val_reg = val_df_processed[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_columns(data_df, cols):\n",
    "    \"\"\"Applies a log transform to `cols` in `data_df`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame in which the columns will be transformed.\n",
    "    cols: collection\n",
    "        The columns in `data_df` to be log scaled.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after log scaling\n",
    "        the columns `cols`.\n",
    "    \n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        data_df[col] = data_df[col].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-88fb59fe720e>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[col] = data_df[col].apply(lambda x: np.log(x) if x > 0 else 0)\n"
     ]
    }
   ],
   "source": [
    "log_transform_cols = ['text_reviews_count', 'ratings_count']\n",
    "X_train_reg = log_transform_columns(X_train_reg, log_transform_cols)\n",
    "X_val_reg = log_transform_columns(X_val_reg, log_transform_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_reg = min_max_scaler.fit_transform(X_train_reg)\n",
    "X_val_reg = min_max_scaler.transform(X_val_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Textual Model\n",
    "\n",
    "We apply TF-IDF only to the unique corpus of book descriptions. We only want to apply it to the unique descriptions because we do not want to overweight the books that are frequently read.\n",
    "\n",
    "We start by getting the book corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = train_df_processed[['book_id', 'cleaned_text']]\n",
    "book_df = book_df.drop_duplicates(subset=['book_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we fit TF-IDF to the book corpus and then use it to transform the training and validation text to a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "\n",
    "tfidf_model.fit(book_df['cleaned_text'])\n",
    "\n",
    "train_tfidf = tfidf_model.transform(train_df_processed['cleaned_text'])\n",
    "val_tfidf = tfidf_model.transform(val_df_processed['cleaned_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
