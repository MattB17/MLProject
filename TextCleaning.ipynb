{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DATA_DIR = \"./output_data/\"\n",
    "\n",
    "train_df = pd.read_csv(OUTPUT_DATA_DIR+\"interactions_training.csv\")\n",
    "\n",
    "val_df = pd.read_csv(OUTPUT_DATA_DIR+\"interactions_validation.csv\")\n",
    "\n",
    "test_df = pd.read_csv(OUTPUT_DATA_DIR+\"interactions_testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Matthew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def process_book_text(book_text, exclude_text, ps):\n",
    "    \"\"\"Pre-processes the text given by `review_text`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    book_text: str\n",
    "        The book text to be processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to be excluded.\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used to perform word stemming.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A string representing the processed version of `review_text`.\n",
    "\n",
    "    \"\"\"\n",
    "    book = re.sub('[^a-zA-Z0-9]', ' ', book_text).lower().split()\n",
    "    book = [ps.stem(word) for word in book if not word in exclude_text]\n",
    "    return ' '.join(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_classification(data_df):\n",
    "    \"\"\"Preprocesses `data_df` to be used in classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame to be processed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after processing.\n",
    "\n",
    "    \"\"\"\n",
    "    # flags for most popular formats\n",
    "    data_df['format'] = data_df['format'].apply(lambda x: str(x).lower())\n",
    "    data_df['is_paperback'] = data_df['format'].apply(lambda x: int(\"paper\" in x))\n",
    "    data_df['is_hardcover'] = data_df['format'].apply(lambda x: int(\"hard\" in x))\n",
    "    data_df['is_audio'] = data_df['format'].apply(lambda x: int(\"audio\" in x))\n",
    "    data_df['is_other_format'] = (data_df['is_paperback'] + data_df['is_hardcover'] +\n",
    "                                  data_df['is_audio'] + data_df['is_ebook'])\n",
    "    data_df['is_other_format'] = data_df['is_other_format'].apply(lambda x: 0 if x > 0 else 1)\n",
    "\n",
    "    #flags for most popular publishers\n",
    "    data_df['publisher'] = data_df['publisher'].apply(lambda x: str(x).lower())\n",
    "    data_df['from_penguin'] = data_df['publisher'].apply(lambda x: int(\"penguin\" in x))\n",
    "    data_df['from_harpercollins'] = data_df['publisher'].apply(lambda x: int(\"harpercollins\" in x or \"harper collins\" in x))\n",
    "    data_df['from_university_press'] = data_df['publisher'].apply(lambda x: int(\"university press\" in x))\n",
    "    data_df['from_vintage'] = data_df['publisher'].apply(lambda x: int(\"vintage\" in x))\n",
    "    data_df['from_createspace'] = data_df['publisher'].apply(lambda x: int(\"createspace\" in x or \"create space\" in x))\n",
    "    data_df['other_publisher'] = (data_df['from_penguin'] + data_df['from_harpercollins'] +\n",
    "                                  data_df['from_university_press'] + data_df['from_vintage'] + data_df['from_createspace'])\n",
    "    data_df['other_publisher'] = data_df['other_publisher'].apply(lambda x: 0 if x > 0 else 1)\n",
    "\n",
    "    # ensuring columns are not missing\n",
    "    data_df['average_rating'] = data_df['average_rating'].apply(lambda x: 0.0 if pd.isnull(x) else x)\n",
    "    train_df['text_reviews_count'] = data_df['text_reviews_count'].apply(lambda x: 0 if pd.isnull(x) else x)\n",
    "    data_df['ratings_count'] = data_df['ratings_count'].apply(lambda x: 0 if pd.isnull(x) else x)\n",
    "    median_page_count = data_df['num_pages'].median()\n",
    "\n",
    "    data_df['num_pages'] = data_df['num_pages'].apply(lambda x: median_page_count if pd.isnull(x) else x)\n",
    "\n",
    "    # flags for most popular authors\n",
    "    data_df['main_author'] = data_df['main_author'].astype(str)\n",
    "    data_df['author_a'] = data_df['main_author'].apply(lambda x: int(x == \"435477.0\"))\n",
    "    data_df['author_b'] = data_df['main_author'].apply(lambda x: int(x == \"903.0\"))\n",
    "    data_df['author_c'] = data_df['main_author'].apply(lambda x: int(x == \"947.0\"))\n",
    "    data_df['author_d'] = data_df['main_author'].apply(lambda x: int(x == \"4624490.0\"))\n",
    "    data_df['author_e'] = data_df['main_author'].apply(lambda x: int(x == \"18540.0\"))\n",
    "    data_df['author_f'] = data_df['main_author'].apply(lambda x: int(x == \"8075577.0\"))\n",
    "    data_df['author_other'] = (data_df['author_a'] + data_df['author_b'] +\n",
    "                                data_df['author_c'] + data_df['author_d'] +\n",
    "                                data_df['author_e'] + data_df['author_f'])\n",
    "    data_df['author_other'] = data_df['author_other'].apply(lambda x: 0 if x > 0 else 1)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_book_text(data_df, id_col, text_col, exclude_text, ps):\n",
    "    \"\"\"Preprocesses the book text in `data_df` for `text_col`.\n",
    "\n",
    "    The dataframe is restricted to `id_col` and `text_col` and then the\n",
    "    unique ids are chosen. This is so that we only preprocess the text\n",
    "    for a book once. Then we join the resulting text back to `data_df`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        The DataFrame containing the data to be preprocessed.\n",
    "    id_col: str\n",
    "        The column from which unique ids are chosen.\n",
    "    text_col: str\n",
    "        The column to be pre-processed.\n",
    "    exclude_text: collection\n",
    "        A collection of words to remove\n",
    "    ps: PorterStemmer\n",
    "        The PorterStemmer used for word stemming.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame obtained from `data_df` after adding a column\n",
    "        with the processed text.\n",
    "\n",
    "    \"\"\"\n",
    "    book_df = data_df[[id_col, text_col]]\n",
    "    book_df = book_df.drop_duplicates(subset=[id_col])\n",
    "    book_df['cleaned_text'] = book_df[text_col].apply(lambda x: process_book_text(x, exclude_text, ps))\n",
    "    final_df = pd.merge(data_df, book_df[[id_col, \"cleaned_text\"]], how=\"inner\", on=[id_col])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocess_pipeline(data_df, exclude_text, ps):\n",
    "    \"\"\"Runs the full pre-processing pipeline on `data_df`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    processed_df = preprocess_for_classification(data_df)\n",
    "    return preprocess_all_book_text(processed_df, \"book_id\", \"title_description\", exclude_text, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_english = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "train_df_processed = run_preprocess_pipeline(train_df, exclude_english, ps)\n",
    "val_df_processed = run_preprocess_pipeline(val_df, exclude_english, ps)\n",
    "test_df_processed = run_preprocess_pipeline(test_df, exclude_english, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>is_read</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text_incomplete</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>...</th>\n",
       "      <th>from_createspace</th>\n",
       "      <th>other_publisher</th>\n",
       "      <th>author_a</th>\n",
       "      <th>author_b</th>\n",
       "      <th>author_c</th>\n",
       "      <th>author_d</th>\n",
       "      <th>author_e</th>\n",
       "      <th>author_f</th>\n",
       "      <th>author_other</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55e33e664f052f3858d8df5d9cd38597</td>\n",
       "      <td>2547</td>\n",
       "      <td>a3a8cf3cead1f647c240ce56d394090d</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Oct 18 14:39:29 -0700 2013</td>\n",
       "      <td>Fri Oct 18 14:39:29 -0700 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>prophet kahlil gibran masterpiec prophet one b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b895142b8e9211781797fb5183db1607</td>\n",
       "      <td>2547</td>\n",
       "      <td>29ac8c961f92d37924d9baaaf82346bf</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Feb 02 07:44:33 -0800 2014</td>\n",
       "      <td>Sun Feb 02 07:44:33 -0800 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>prophet kahlil gibran masterpiec prophet one b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b803e1294962dee7ba2fa786cc784e2</td>\n",
       "      <td>2547</td>\n",
       "      <td>27ba784279300ef5bbe2f6aa087324bc</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thu Aug 04 06:43:11 -0700 2011</td>\n",
       "      <td>Thu Aug 04 06:43:11 -0700 2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>prophet kahlil gibran masterpiec prophet one b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e159aee0ab026edbbcfae81b36efe72c</td>\n",
       "      <td>2547</td>\n",
       "      <td>0f95079de88cde4e024a26c8896e590d</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Dec 08 19:37:35 -0800 2014</td>\n",
       "      <td>Mon Dec 08 19:37:35 -0800 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>prophet kahlil gibran masterpiec prophet one b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b1b585d4de9e3524c98f12554b9c56b4</td>\n",
       "      <td>2547</td>\n",
       "      <td>e904e32673dc13d8a35c53e571fcf11e</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Oct 22 21:43:23 -0700 2017</td>\n",
       "      <td>Sun Oct 22 21:43:23 -0700 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>prophet kahlil gibran masterpiec prophet one b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257640</th>\n",
       "      <td>83d4c1428afe4b0df30af8fc55bd273c</td>\n",
       "      <td>237611</td>\n",
       "      <td>31bbe3f2f4006a5c02ec013ef0099c9d</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Sep 15 13:14:10 -0700 2009</td>\n",
       "      <td>Tue Sep 15 13:14:10 -0700 2009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>translat neruda way macchu picchu goe translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257641</th>\n",
       "      <td>8d78dea6e0bb399718a0a02a8e7285d4</td>\n",
       "      <td>18329474</td>\n",
       "      <td>fc4c11a914886a0389b1e2f37a5ec893</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>\" Even your Weakness&lt;br /&gt;,,Can be a strength&lt;...</td>\n",
       "      <td>Fri Sep 13 04:24:02 -0700 2013</td>\n",
       "      <td>Tue Dec 03 11:19:55 -0800 2013</td>\n",
       "      <td>Sun Nov 24 00:00:00 -0800 2013</td>\n",
       "      <td>Fri Sep 13 00:00:00 -0700 2013</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>word boat word thought help other simpl word c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257642</th>\n",
       "      <td>15ab66649268d565e97f3c1c58df4563</td>\n",
       "      <td>32934749</td>\n",
       "      <td>e2f79ac4f775c948c4a051a52cd347e7</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Jun 02 04:55:12 -0700 2017</td>\n",
       "      <td>Fri Jun 02 04:55:12 -0700 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>select poem emili dickinson born promin new en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257643</th>\n",
       "      <td>1a8db358578612605b1b3dbf83a3ed75</td>\n",
       "      <td>19017083</td>\n",
       "      <td>2df89dec3f0350a8e015280cbec4c7d4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wed Jan 29 13:15:54 -0800 2014</td>\n",
       "      <td>Wed Jan 29 13:15:54 -0800 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>gather evid pinpoint accuraci virtuos humour c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257644</th>\n",
       "      <td>94ec99accc8b7045417b1903377a5e7b</td>\n",
       "      <td>17265871</td>\n",
       "      <td>5291f0c8494de803ba3f1b854ca49060</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sat Dec 10 13:40:48 -0800 2016</td>\n",
       "      <td>Sat Dec 10 13:40:49 -0800 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>leda swan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257645 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_id   book_id  \\\n",
       "0       55e33e664f052f3858d8df5d9cd38597      2547   \n",
       "1       b895142b8e9211781797fb5183db1607      2547   \n",
       "2       1b803e1294962dee7ba2fa786cc784e2      2547   \n",
       "3       e159aee0ab026edbbcfae81b36efe72c      2547   \n",
       "4       b1b585d4de9e3524c98f12554b9c56b4      2547   \n",
       "...                                  ...       ...   \n",
       "257640  83d4c1428afe4b0df30af8fc55bd273c    237611   \n",
       "257641  8d78dea6e0bb399718a0a02a8e7285d4  18329474   \n",
       "257642  15ab66649268d565e97f3c1c58df4563  32934749   \n",
       "257643  1a8db358578612605b1b3dbf83a3ed75  19017083   \n",
       "257644  94ec99accc8b7045417b1903377a5e7b  17265871   \n",
       "\n",
       "                               review_id  is_read  rating  \\\n",
       "0       a3a8cf3cead1f647c240ce56d394090d     True       5   \n",
       "1       29ac8c961f92d37924d9baaaf82346bf     True       4   \n",
       "2       27ba784279300ef5bbe2f6aa087324bc     True       5   \n",
       "3       0f95079de88cde4e024a26c8896e590d     True       4   \n",
       "4       e904e32673dc13d8a35c53e571fcf11e    False       0   \n",
       "...                                  ...      ...     ...   \n",
       "257640  31bbe3f2f4006a5c02ec013ef0099c9d    False       0   \n",
       "257641  fc4c11a914886a0389b1e2f37a5ec893     True       5   \n",
       "257642  e2f79ac4f775c948c4a051a52cd347e7     True       3   \n",
       "257643  2df89dec3f0350a8e015280cbec4c7d4    False       0   \n",
       "257644  5291f0c8494de803ba3f1b854ca49060    False       0   \n",
       "\n",
       "                                   review_text_incomplete  \\\n",
       "0                                                     NaN   \n",
       "1                                                     NaN   \n",
       "2                                                     NaN   \n",
       "3                                                     NaN   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "257640                                                NaN   \n",
       "257641  \" Even your Weakness<br />,,Can be a strength<...   \n",
       "257642                                                NaN   \n",
       "257643                                                NaN   \n",
       "257644                                                NaN   \n",
       "\n",
       "                            date_added                    date_updated  \\\n",
       "0       Fri Oct 18 14:39:29 -0700 2013  Fri Oct 18 14:39:29 -0700 2013   \n",
       "1       Sun Feb 02 07:44:33 -0800 2014  Sun Feb 02 07:44:33 -0800 2014   \n",
       "2       Thu Aug 04 06:43:11 -0700 2011  Thu Aug 04 06:43:11 -0700 2011   \n",
       "3       Mon Dec 08 19:37:35 -0800 2014  Mon Dec 08 19:37:35 -0800 2014   \n",
       "4       Sun Oct 22 21:43:23 -0700 2017  Sun Oct 22 21:43:23 -0700 2017   \n",
       "...                                ...                             ...   \n",
       "257640  Tue Sep 15 13:14:10 -0700 2009  Tue Sep 15 13:14:10 -0700 2009   \n",
       "257641  Fri Sep 13 04:24:02 -0700 2013  Tue Dec 03 11:19:55 -0800 2013   \n",
       "257642  Fri Jun 02 04:55:12 -0700 2017  Fri Jun 02 04:55:12 -0700 2017   \n",
       "257643  Wed Jan 29 13:15:54 -0800 2014  Wed Jan 29 13:15:54 -0800 2014   \n",
       "257644  Sat Dec 10 13:40:48 -0800 2016  Sat Dec 10 13:40:49 -0800 2016   \n",
       "\n",
       "                               read_at                      started_at  ...  \\\n",
       "0                                  NaN                             NaN  ...   \n",
       "1                                  NaN                             NaN  ...   \n",
       "2                                  NaN                             NaN  ...   \n",
       "3                                  NaN                             NaN  ...   \n",
       "4                                  NaN                             NaN  ...   \n",
       "...                                ...                             ...  ...   \n",
       "257640                             NaN                             NaN  ...   \n",
       "257641  Sun Nov 24 00:00:00 -0800 2013  Fri Sep 13 00:00:00 -0700 2013  ...   \n",
       "257642                             NaN                             NaN  ...   \n",
       "257643                             NaN                             NaN  ...   \n",
       "257644                             NaN                             NaN  ...   \n",
       "\n",
       "       from_createspace  other_publisher author_a author_b author_c author_d  \\\n",
       "0                     0                1        0        0        0        0   \n",
       "1                     0                1        0        0        0        0   \n",
       "2                     0                1        0        0        0        0   \n",
       "3                     0                1        0        0        0        0   \n",
       "4                     0                1        0        0        0        0   \n",
       "...                 ...              ...      ...      ...      ...      ...   \n",
       "257640                0                0        0        0        0        0   \n",
       "257641                0                1        0        0        0        0   \n",
       "257642                0                1        0        0        0        0   \n",
       "257643                0                1        0        0        0        0   \n",
       "257644                0                1        0        0        0        0   \n",
       "\n",
       "       author_e  author_f  author_other  \\\n",
       "0             0         0             1   \n",
       "1             0         0             1   \n",
       "2             0         0             1   \n",
       "3             0         0             1   \n",
       "4             0         0             1   \n",
       "...         ...       ...           ...   \n",
       "257640        0         0             1   \n",
       "257641        0         0             1   \n",
       "257642        0         0             1   \n",
       "257643        0         0             1   \n",
       "257644        0         0             1   \n",
       "\n",
       "                                             cleaned_text  \n",
       "0       prophet kahlil gibran masterpiec prophet one b...  \n",
       "1       prophet kahlil gibran masterpiec prophet one b...  \n",
       "2       prophet kahlil gibran masterpiec prophet one b...  \n",
       "3       prophet kahlil gibran masterpiec prophet one b...  \n",
       "4       prophet kahlil gibran masterpiec prophet one b...  \n",
       "...                                                   ...  \n",
       "257640  translat neruda way macchu picchu goe translat...  \n",
       "257641  word boat word thought help other simpl word c...  \n",
       "257642  select poem emili dickinson born promin new en...  \n",
       "257643  gather evid pinpoint accuraci virtuos humour c...  \n",
       "257644                                          leda swan  \n",
       "\n",
       "[257645 rows x 72 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_texts = train_df_processed[['book_id', 'cleaned_text']]\n",
    "book_texts = book_texts.drop_duplicates(subset=['book_id'])\n",
    "\n",
    "full_text = \" \".join(list(book_texts['cleaned_text'])) + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "word_cloud = WordCloud(width=800, height=800,\n",
    "                       background_color='white',\n",
    "                       min_font_size=10).generate(full_text)\n",
    "\n",
    "plt.figure(figsize=(8,8), facecolor=None)\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df_processed) + len(val_df_processed) + len(test_df_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "interactions_file = os.path.join(\"data\", \"goodreads_interactions_poetry.json\")\n",
    "\n",
    "users = []\n",
    "with open(interactions_file, 'r') as data_file:\n",
    "    for row in data_file:\n",
    "        users.append(json.loads(row)['user_id'])\n",
    "\n",
    "np_users = np.array(users)\n",
    "del users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.unique(np_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_dataset(data_df):\n",
    "    \"\"\"Randomly shuffles `df`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The DataFrame to be shuffled.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A shuffled dataframe obtained from `df`.\n",
    "\n",
    "    \"\"\"\n",
    "    data_df = shuffle(data_df)\n",
    "    data_df.reset_index(inplace=True, drop=True)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_processed = shuffle_dataset(train_df_processed)\n",
    "val_df_processed = shuffle_dataset(val_df_processed)\n",
    "test_df_processed = shuffle_dataset(test_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_processed.to_csv(OUTPUT_DATA_DIR+\"text_processed_training.csv\", index=False)\n",
    "val_df_processed.to_csv(OUTPUT_DATA_DIR+\"text_processed_validation.csv\", index=False)\n",
    "test_df_processed.to_csv(OUTPUT_DATA_DIR+\"text_processed_testing.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc2515-env-3.8",
   "language": "python",
   "name": "csc2515-env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
